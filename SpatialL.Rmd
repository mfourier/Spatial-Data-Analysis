---
title: "Modelos de predicción de precios para datos georreferenciados"
author: "Maximiliano S. Lioi"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
bibliography: "references.bib"
csl: acm-sig-proceedings-long-author-list.csl
fontsize: 11pt
geometry: margin=1in
---

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(tidycensus)
library(tidyr)
library(tidyverse)
library(censusapi)
library(tmap)
library(ggplot2)
library(dplyr)
library(stringr)
library(units)
library(stats)
library(grDevices)
library(dotenv)
library(sf)
library(corrr)
library(spatialreg)
library(spdep)
library(jtools)
library(huxtable)
library(GWmodel)
library(spgwr)
library(SpatialML)
library(ggthemes)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

El objetivo de predecir el precio de las viviendas en una determinada ubicación utilizando datos georreferenciados trae consigo una serie de desafíos que debemos enfrentar, tales como: la autocorrelación espacial y la heterogeneidad espacial sobre las muestras; estas propiedades afectan suposiciones clásicas para asegurar la calidad de la predicción en modelos más simples de regresión, como puede ser la identica e independiente distribución de los datos o la homocedasticidad de estos.

El presente documento se enfoca en el desarrollo de modelos de regresión espacial, los cuales adaptan la naturaleza espacial de los datos en sus formulaciones, tomar estas cualidades en cuenta permite obtener modelos más precisos, estos resultados pueden ser útiles para diversos ámbitos de la industria, en cuyas tomas de decisiones intervengan datos que están asociados a una componente geográfica, como es el caso de la predicción de precios de vivienda dentro del mercado inmobiliario.

En el contexto de datos espaciales [@8444678], trabajamos con un conjunto de observaciones de datos asociados con una componente espacial $\{(x(s_i),y(s_i)) | \ i \in \mathbb{N} , 1 \leq i \leq n\}$ donde $n \in \mathbb{N}$ es la cantidad de muestras, $s_i \in \mathbb{R}^{2}$ es un vector de coordenadas espaciales, $x(s_i) \in \mathbb{R}^m$ son las variables explicativas del modelo e $y(s_i) \in \mathbb{R}$ es la respuesta a dichas variables.

Este mismo conjunto también puede ser descrito en formato matricial como $(X,Y) \in \mathbb{R}^{(m+1) \times n}$ donde $X = [x(s_1), x(s_2), \dots, x(s_n)]^{T} \in \mathbb{R}^{m \times n}$ e $Y = [y(s_1), y(s_2), \dots, y(s_n)]^{T} \in \mathbb{R}^{n}$.

El problema de la predicción consiste en que, dado un conjunto de muestras de data espacial, buscamos modelar una función $f$ tal que $Y = f(X) + \varepsilon$ donde $\varepsilon$ es un término para el error, una vez que el modelo se entrena, minimizando el error, puede usarse para precedir la respuesta en otra locación dada sus variables explicativas.

La predicción espacial se diferencia de la predicción usual, puesto que en este último, se asume que las muestras son independientes e identicamente distribuidas (i.i.d) y por lo tanto, dado un modelo entrenado en la función $f$, podemos usarlo para predecir $y(s) = f(x(s))$ para todo $s$, sin embargo, la suposición de que los datos son i.i.d no se cumple al trabajar con datos espaciales, esto debido a la relación implícita que existe entre posiciones cercanas en una región.

Acorde a la primera ley de la geografía de Tobler W.R: "Todo se relaciona con todo lo demás, pero las cosas más cercanas se relacionan más que las cosas distantes" [@tobler1970], este fenómeno se conoce como autocorrelación espacial, e ignorarlo puede afectar la precisión de modelos que tengan el supuesto de una muestra i.i.d, pues perdemos esta propiedad sobre los datos.

\newpage

Respecto de los desafíos presentes asociados a la predicción espacial, abordamos los siguientes a lo largo del documento:

-   $\textit{Autocorrelación espacial}$ $\rightarrow$ como indica Tobler con su primera ley de geografía, los datos espaciales no son estadísticamente independientes entre sí, al contrario estos, están correlacionados, y la intensidad de dicha relación depende de la distancia que exista entre estos, siendo mayor cuando los datos están cerca entre sí, dicho fenómeno interviene cuando usamos modelos de predicción que toman por hipótesis una distribución independiente e identica entre los datos, como es el caso de la regresión lineal.

-   $\textit{Heterogeneidad espacial}$ $\rightarrow$ cuando fragmentamos el espacio para el análisis de los datos y le asignamos a cada sector algún atributo, los datos que conforman cada zona no siguen una distribución idéntica, modelos clásicos que no toman en cuenta este fenómeno, por medio de la función que represente al proceso, los valores que representan la relación entre los regresores y la variable dependiente se asumen constantes para los regresores independiente de su posición geográfica, esto se conoce como estacionariedad. Dichas suposiciones permiten simplificar la creación de modelos, sin embargo, cuando no se cumple este principio, como es típico con datos demográficos, puede interferir en la precisión y es por ello que necesitamos técnicas que tomen en cuenta las variaciones locales.

Para el análisis de datos haremos uso de $\textit{tidycensus}$, paquete de R diseñado para facilitar el proceso de adquisición y manejo de datos sobre la población, proporcionados por la oficina del censo de EE.UU, acompañados de otros paquetes para el uso de modelos de predicción, visualización, entre otros.

En $\textit{Analyzing US Census Data - Kyle Walker}$ se muestra como hacer uso de las librerías, entre ellas:

-   tidycensus : Paquete de R diseñado para facilitar procesos de adquirir y trabajar data de US Census, busca distribuir los datos del censo en un formado compatible con tidyverse, además busca agilizar el proceso del tratado de datos para aquel que esté trabajando en el analisis de datos. [Ch2.](https://walker-data.com/census-r/an-introduction-to-tidycensus.html)

-   *tidyverse* : Coleción de paquetes de R diseñados para la ciencia de datos tales como *ggplot2* para la visualización de data, *readr* para importar y exportar bases de datos, *tidyr* para la remodelación de datos, entre otros. [Ch3.](https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html)

-   *tigris* : Paquete de R que busca simplificar procesos para los usuarios de obtención de información y uso de data con atributos geográficos (data espacial, Census geographic dataset), data tipo *sf* (simple features) viene con atributos de geometría (vector data type, tipicamente representados por puntos lineas o poligonos). [Ch5.](https://walker-data.com/census-r/census-geographic-data-and-applications-in-r.html)

-   *ggplot2* : Paquete de R enfocado en la visualización de data, nos permite realizar mapas con información de US Census data. [Ch6.](https://walker-data.com/census-r/mapping-census-data-with-r.html)

-   *spatialreg & GWmodel* : Paquete diseñado para la aplicación de modelos de regresión espacial

\newpage

# Importando datos

Se importan datos con *tidycensus* provenientes de la American Community Survey (ACS) (2016-2020), haciendo uso de la función get_acs()

Tomamos datos con nivel geográfico de distritos para la ciudad de NY, para ello pedimos datos de los condados de: 'Bronx', 'Kings', 'New York', 'Queens', 'Richmond'

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
#package
library(dotenv)
library(sf)
library(tidycensus)
library(dplyr)

# Variable: Condados que forman NYC
nyc_counties <- c("Bronx","Kings","New York","Queens","Richmond")

# Lista de regresores a utilizar
variables_to_get <- c(
  median_value = "B25077_001",
  median_rooms = "B25018_001",
  median_income = "DP03_0062",
  total_population = "B01003_001",
  median_age = "B01002_001",
  pct_college = "DP02_0068P",
  pct_foreign_born = "DP02_0094P",
  pct_white = "DP05_0077P",
  pct_black = "DP05_0078P",
  pct_hispanic = "DP05_0070P",
  pct_asian = "DP05_0080P",
  percent_ooh = "DP04_0046P"
)
```

```{r, results='hide', warning=FALSE, message=FALSE}
# Obtiene datos de la ACS y transforma a tipo NYC EPSG
nyc_data <- get_acs(
  geography = "tract",
  variables = variables_to_get,
  state = "NY",
  county = nyc_counties,
  geometry = TRUE,
  output = "wide",
  year = 2020,
  key = Sys.getenv("CENSUS_API")) %>% 
  st_transform(2263)

```

Las variables que importamos, para usar en los distintos modelos de predicción, son las siguientes:

-   **median_value** : El valor medio de la vivienda del tramo censal, nuestra variable a predecir

-   median_rooms : Cantidad media de habitaciones por casa en el tramo censal

-   total_population : Población total en el tramo censal

-   median_age : Edad media de la población en el tramo censal

-   median_year_built : Año promedio donde se construyó la vivienda

-   median_income : Ingreso medio de los hogares en el tramo censal

-   pct_college : Porcentaje de la población de 25 años o más con un título universitario de cuatro años

-   pct_foreign_born: Porcentaje de la población que nació fuera de EE.UU

-   pct_white : Porcentaje de la población que se identifica como blanco no-hispano, se sigue la misma lógica con pct_black, pct_asian, pct_hispanic.

-   percent_ooh : Porcentaje de viviendas en el tramo censal que están siendo ocupadas por sus propietarios.

Los detalles para descargar información de las distintas bases de datos que proporciona *tidycensus* y las distintas variables que tenemos a disposición se encuentran en el **Anexo 3**.

Más detalles sobre la estructura del código se encuentran en la sección 8.2.1 de *Analyzing US Census Data*, en el libro se toma la misma variable "median home value", pero a diferencia de nuestro caso, se toman los tramos censales en Dallas-Fort Worth metropolitan area, en consecuencia también cambian el código usando en st_transform, a un sistema de referencias apropiado para North Texas (code 32148), a diferencia del caso NYC donde usamos code 2263

\newpage

# Preparando la data para modelar

Se realiza una limpieza de las muestras (data scrubbing), identificando datos incompletos, incorrectos o inexactos.

Visualizamos un resumen de los datos, mostrando sus estadísticos principales, tales como mínimo, máximo, promedio, mediana, cuartiles y valores perdidos.

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
resumen_nyc <- data.frame(unclass(summary(nyc_data)), check.names = FALSE, stringsAsFactors = FALSE)
resumen_nyc <- resumen_nyc[,-1:-2]
resumen_nyc
```

```{r, echo = FALSE, fig.align = 'center', out.width="100%"}
knitr::include_graphics("image/estadisticas.png")
```

La recolección de datos incluye para cada variable otra columna asociada que representa el margen de error de los datos (aquellas que terminan en 'M'), sin embargo, no haremos uso de ello y por tanto la eliminamos del modelo.

Nuestra variable dependiente para los modelos de regresión será el valor promedio de la vivienda en NYC (median_value).

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE, out.width='80%', fig.align = 'center'}
# Plot tmap
(nyc_median_value_hist_tm <- nyc_data[!st_is_empty(nyc_data),,drop=F] %>% 
tm_shape() +
  tm_polygons(col = "median_valueE",
          palette = "cividis",
          title = "2016-2020 ACS",
          legend.hist = TRUE,
          legend.format = scales::dollar_format()) +
  tm_layout(main.title = "NYC Median Home Value",
            frame = FALSE,
            legend.outside = TRUE,
            bg.color = "grey100",
            legend.hist.width = 5,
            ))

# Guarda Plot
tmap_save(nyc_median_value_hist_tm, "image/nyc_median_value.png", width=1920, height=1080, asp=0)
```

\newpage

El histograma presenta una asimetría a la derecha, i.e, existe una población no menor cuyas viviendas poseen un valor muy alejado de la mediana, usualmente es más facil trabajar con la variable dependiente distribuida normal, por sus buenas propiedades y teoremas asociados, además que en la práctica, permite resolver problemas como la normalidad de los residuos, para ello podemos aplicar una transformación raíz cuadrada, por otro lado tenemos datos sin información, para esto aplicamos herramientas de R para el filtrado.

La distribución de la variable dependiente aplicada esta transformación se ve de la siguiente manera

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE, out.width='70%', fig.align = 'center'}

# Creamos plot
(nyc_median_value_sqrt_hist_tm <- nyc_data[!st_is_empty(nyc_data),,drop=F] %>%
   
# Limpiamos datos vacios
mutate(sqrt_med_value = sqrt(median_valueE)) %>% #variable_sqrt
tm_shape() +
  tm_polygons(col = "sqrt_med_value",
          palette = "cividis",
          title = "2016-2020 ACS (sqrt)",
          legend.hist = TRUE) +
  tm_layout(main.title = "NYC Median Home Value",
            frame = FALSE,
            legend.outside = TRUE,
            bg.color = "grey100",
            legend.hist.width = 5,
            ))

# Guarda Plot
tmap_save(nyc_median_value_sqrt_hist_tm, "image/nyc_median_value_sqrt.png", width=1920, height=1080, asp=0)
```

Eliminamos aquellas variables con data NA, y aquellas columnas con información del margen de error, por lo demás, como tenemos la cantidad de población, y el área que cubre cada distrito (polygon), creamos la variable pop_density, esto pues buscamos transformar la información de la población por distrito de manera que represente mejor la relación que tiene con la variable de salida, en este caso, el valor medio de la vivienda

-   pop_density $\rightarrow$ mide densidad de población en el tramo censal por metros cuadrados

```{r, warning=FALSE, message=FALSE}
# Preparando la data
nyc_data_prepped <- nyc_data %>% 
  mutate(pop_density = as.numeric(set_units(total_populationE / st_area(.),
 "1/km2"))) %>% 
  select(!ends_with("M")) %>% # Eliminamos columnas de margen de error
  rename_with(.fn = ~str_remove(.x, "E$")) %>% 
  na.omit() # Elimina valores NA

```

Para ver como visualizar los datos haciendo uso de la geometría incorporada por *tidycensus*, ver **Anexo 4** en donde se hace una introducción de los paquetes y la sintaxis.

\newpage

# Autocorrelación espacial: Efectos sobre una regresión lineal

Existen múltiples modelos estadísticos que se rigen por hipótesis que requieren de una distribución i.i.d de los datos, sin embargo, como mencionamos anteriormente, esta cualidad se pierde al trabajar con datos espaciales.

En esta sección, se presenta un estadístico para medir la autocorrelación espacial presente en los datos, interpretaciones de dicho estadístico y como este fenómeno afecta la predicción de modelos, cuyas hipótesis se ven comprometidas por la naturaleza de la muestra, como es el caso de la regresión lineal

## Índice de Moran

Para medir la autocorrelación espacial presente en los datos, hacemos uso del índice de Moran $I$ [@8444678] [@nikparvar2021], este coeficiente nos entrega una magnitud de la intensidad de este fenómeno para $n$ observaciones de una misma variable, donde las muestras poseen una componente espacial.

Definimos el índice de Moran como $$I = \frac{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} (y(s_i) - \bar{y})(y(s_j) - \bar{y})}{(\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij}) \sum _{i=1}^{n} (y(s_i) - \bar{y})^2 /n }$$
O de manera equivalente 
$$= \frac{n}{W}\frac{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} (y(s_i) - \bar{y})(y(s_j) - \bar{y})}{\sum _{i=1}^{n} (y(s_i) - \bar{y})^2}$$
donde $\bar{y} = \sum_{i=1}^{n} y(s_i)/n$ con $n$ el número total de muestras, a partir de ahora, para simplificar la notación, $y(s_i) = y_i$ dejando implicita la componente espacial, además $W = \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij}$, y de forma abreviada, $W = (w_{ij})_{i,j = 1}^{n}$, que consideramos como una $\textbf{matriz de pesos estandarizada}$.

Entenderemos por $\textbf{matriz de pesos estandarizada}$ aquella que cumple las siguientes propiedades

-   Supondremos $w_{ij} \geq 0 \ \  \forall i,j \in \{1, \dots, n\}$, pues $w\_{ij}$ es una magnitud de la influencia que hay entre los vecinos $i$ y $j$

-   Típicamente, $w_{ii} = 0 \ \ \forall i \in \{1, \dots, n\}$, pues el hecho de que un punto tenga influencia con el mismo puede generar ruido en el modelo

-   Suponemos que la influencia que tiene $y_{i}$ sobre $y_{j}$ es la misma que la de $y_{j}$ sobre $y_{i}$, esto es, $w_{ij} = w_{ji} \ \ \forall i,j \in \{1, \dots, n\}$, es decir, $W$ es simétrica

-   *Row-normalized*: Para cada $i \in \{1,\dots,n\}$ se tiene que $\sum_{j=1}^{n} w_{ij} = \alpha$ para algún $\alpha \in \mathbb{R}$ esto pues, entendemos la suma $\sum_{j=1}^{n} w_{ij}$ como la influencia total que existe entre el vecino $i$ y todos sus vecinos $j$, por lo que pedimos que la influencia total sea la misma para cada vecino $i \in \{1,\dots,n\}$, de manera que el nivel de influencia esté estandarizado para cada vecino, típicamente tomamos $\alpha = 1$.

El índice de Moran, toma valores entre -1 y 1 dado un muestreo aleatorio $(y(s_i))_{i=1}^{n}$, para una demostración detallada de esta propiedad, ver **Anexo 5**.

\newpage

Este estadístico mide que tan similar es una variable respecto de las variables cercanas, por ejemplo, si este se acerca a $-1$ nos dice que cada en cada región, esta tiende a tener valores distinto de su entorno, y en caso contrario, si se acerca a $1$, cada región es similar a su entorno, resumiendo

-   $I \approx -1 \rightarrow$ Se clusterizan valores distintos entre sí (dispersión perfecta)

-   $I \approx 0 \rightarrow$ No existe una clara relación entre los valores de los datos y sus entornos, es decir, no hay pruebas de autocorrelación espacial.

-   $I \approx 1 \rightarrow$ Se clusterizan valores similares entre sí

```{r, echo = FALSE, fig.align = 'center', out.width="80%"}
knitr::include_graphics("image/moranindex.jpg")
```

Comentando sobre el significado de este valor, al igual que la correlación usual, en donde vemos la relación entre 2 variables, tomando una como variable de entrada y la otra su variable de respuesta, la autocorrelación es similar, pero la variable de respuesta es la misma variable de entrada, tomando en cuenta el factor espacial por medio de nuestra matriz de pesos.

Dada su estructura, vemos que el índice de Moran es una auto-covarianza espacial estandarizada [@chen2013].

Además, si consideramos la hipótesis nula:
$$H_0: \text{No existe autocorrelación espacial}$$
La cual consiste en suponer que, dada una permutación de la muestra, es decir, cambiando el orden en el que vienen los datos $(x_i)_{i=1}^{N}$, y fijando la matriz de pesos antes del reordenamiento, no se debiese tener un impacto sobre el valor de $I$, pues estos no muestran relacionarse entre sí, se tiene que

$$\mathbb{E}(I) = -\frac{1}{N-1}$$
Esto es consistente con el modelo, pues para una muestra grande, de no existir autocorrelación espacial, esperamos que $I \approx 0$

Luego, podemos usar esta propiedad para inferencias estadísticas por medio del p-valor

Para una demostración detallada de este hecho, ver **Anexo 5**.

\newpage

## Regresión Lineal

Planteamos un modelo de regresión lineal para los datos importados, dicho modelo no toma en cuenta las componentes espaciales y nuestro objetivo es observar el efecto que esto tiene en la calidad de la precisión

La formulación es la siguiente: $$\sqrt{median \ value} = \alpha + \beta_1(median \ rooms)+ \beta_2(median \ income) +\beta_3(pct \ college)$$
$$ +\beta_4(pct \ foreign \ born) + \beta_5(pct \ white) +\beta_6(median \ age)$$
$$+ \beta_7(percent \ ooh) +\beta_8(pop \ density)+\beta_9(total \ population) + \varepsilon $$
De donde, a partir de $n$ observaciones de la variable dependiente junto a sus regresores, buscamos estimar los valores $\alpha$ y $\beta_i$, mediante mínimos cuadrados, es decir, tenemos para cada observación un error asociado $\varepsilon_i$, y minimizamos $\sum_{i=1}^n \varepsilon_i^2 = \sum_{i=1}^n (y_i - \alpha -\sum_{j=1}^m \beta_j x_{ij})^2$ donde $y_i$ es la observación de la variable independiente, y $x_{ij}$ los $m$ regresores asociados a la observación $i$.

Un modelo de regresión lineal requiere de una serie de hipótesis para asegurar la calidad de las estimaciones de $\beta$ (supuestos de Gauss-Márkov) tales como: 

* **Linearidad**: Los regresores poseen una relación lineal con la variable de respuesta

-   **Independencia**: Las observaciones representan una muestra aleatoria distribuida identica e independiente, de manera que es generalizable para el total de la población

-   **Error con media cero**: Consideramos el error de cada observación $\varepsilon_i$ variable aleatoria tal que $\mathbb{E}(\varepsilon_i) =0$

-   **Normalidad**: Suponemos que $\varepsilon_i$ sigue una distribución normal de la forma $\varepsilon_i \sim \mathcal{N}(0,\sigma^2)$

-   **Homocedasticidad e incorrelación**: Los errores de las observaciones del modelo poseen la misma varianza $\sigma^2$ y son tales que $Cov(\varepsilon_i , \varepsilon_j)$.

Realizando un análisis exploratorio visualizamos una tendencia lineal en los datos entre los regresores y $\sqrt{median \ value}$

```{r, echo=FALSE, message=FALSE, results='hide'}
### transparent colors function
t_col <- function(color, opacity = 0.5) {
  rgb.val <- col2rgb(color)
  t.col <- rgb(rgb.val[1], rgb.val[2], rgb.val[3], max = 255, alpha = (opacity)*255)
  invisible(t.col)
}
```

```{r, echo=FALSE, out.width='60%', fig.align='center'}
# Ploteamos 4 regresores
par(mfrow = c(2, 2))

# Plots
plot(nyc_data_prepped$pct_college, sqrt(nyc_data_prepped$median_value), xlab = "Percentage College (%)", ylab = "Sqrt Median Home Value ($)", col=t_col("black",0.4))
abline(lm(sqrt(nyc_data_prepped$median_value) ~ nyc_data_prepped$pct_college))

plot(nyc_data_prepped$median_income, sqrt(nyc_data_prepped$median_value), xlab = "Median Income ($)", ylab = " Sqrt$ Median Home Value ($)", col=t_col("black",0.4))
abline(lm(sqrt(nyc_data_prepped$median_value) ~ nyc_data_prepped$median_income))

plot(nyc_data_prepped$pct_white, sqrt(nyc_data_prepped$median_value), xlab ="Percentage White (%)",ylab = "Sqrt Median Home Value ($)", col=t_col("black",0.4))
abline(lm(sqrt(nyc_data_prepped$median_value) ~ nyc_data_prepped$pct_white))

plot(nyc_data_prepped$median_rooms, sqrt(nyc_data_prepped$median_value), xlab = "Median Rooms",ylab = "Sqrt Median Home Value ($)", col=t_col("black",0.4))
abline(lm(sqrt(nyc_data_prepped$median_value) ~ nyc_data_prepped$median_rooms))

mtext("Linearidad",                   
      side = 1,
      line = - 2,
      outer = TRUE)
```

\newpage

Además, para el análisis de datos, el paquete *corrr*, nos permite calcular la matriz de correlación asociada [@walker2023a]

```{r, warning=FALSE, message=FALSE}
library(corrr)

nyc_estimates <- nyc_data_prepped %>%
  select(-GEOID, -median_value ) %>%
  st_drop_geometry()

correlations <- correlate(nyc_estimates, method = "pearson")

correlations[is.na(correlations)] <- 0

```

```{r, echo = FALSE, fig.align = 'center', out.width="60%"}
# view model statistics
knitr::include_graphics("image/correlations.png")
```

Y podemos visualizar esta información haciendo uso de network_plot()

```{r, fig.align = 'center', out.width="60%"}
network_plot(correlations)
```

En donde vemos el tipo de correlación que tienen los regresores entre sí.

Se puede realizar un análisis de componentes principales (PCA) con el fin de disminuir la dimensionalidad del problema, en donde reducimos la cantidad de regresores tomando combinaciones lineales de estos, ver *sección 8.2.5: 'Dimension reduction with principal components analysis'* de *Analyzing US Census Data*

\newpage

Para la calibración de los parámetros $\beta$ y $\alpha$ del modelo de regresión lineal, podemos usar la función *lm* que viene incluida con los paquetes básicos del lenguaje R.

```{r, results = 'hide'}
# Fórmula entre variables dependientes y regresores
formula <- "sqrt(median_value) ~ median_rooms + median_income + 
pct_college + pct_foreign_born + pct_white + pct_black + pct_hispanic + 
pct_asian + median_age + percent_ooh + pop_density"

# Regresión lineal mediante lm
model1 <- lm(formula = formula, data = nyc_data_prepped)
summary(model1)
```

```{r, echo = FALSE, fig.align = 'center', out.width="70%"}
# Coeficientes del modelo
knitr::include_graphics("image/lmmodel.png")
```

Haciendo uso de esta función, obtenemos el valor de los coeficientes buscados, el estimado de la variable 'Intercept' se corresponde al valor de $\alpha$ mientras que el estimado del resto de variables se corresponde a los $(\beta_i)_{i=1}^{m}$ asociados.

La columna '$P(>|t|)$' corresponde al **p-valor**, un p-valor alto nos dice que la variable no es significativa en el resultado, típicamente pedimos que sea menor a 0.05 para que la variable se considere significativa, los grados de significancia para el modelo vienen jerarquizados como dice 'Signif. codes', es decir, cuando el p-valor es menor a $0.001$, consideramos que el regresor es altamente significativo en el modelo, menos significativo si el p-valor esta entre $0.001$ y $0.01$ y así sucesivamente.

Aquellas variables con mayor p-value son pct_foreign_born, median_age, pop_density, notamos tambien que las primeras dos variables se correlacionan negativamente con median_value, es decir, a mayor cantidad de nacidos extranjeros y edad promedio, se tiene que el valor medio de la vivienda disminuye. Al contrario, si aumenta la densidad poblacional, tenemos que el valor medio de la vivienta aumenta.

\newpage

Observando el valor $R^2$, coeficiente que mide el porcentaje de varianza de la variable dependiente que es explicado con las variables del modelo, un $R^2= 0.4569$ nos revela que **el modelo es deficiente para la predicción**, una de las razones de esto es debido a la autocorrelación espacial presente en los datos.

Para hacer cuenta de esto último, se estudian los residuos del modelo, que se corresponden a la diferencia entre el valor observado y el valor predicho por el modelo ya entrenado, para ello hacemos uso de la función residuals().

```{r}
#Añadimos los residuos a la data que estamos trabajando
nyc_data_prepped$residuals <- residuals(model1)
```

Mediante un histograma vemos la distribución que siguen los residuos

```{r, echo = FALSE, fig.align = 'center', out.width="85%"}
#Plot
ggplot(nyc_data_prepped, aes(x = residuals)) + 
  geom_histogram(bins = 100, alpha = 0.5, color = "navy",
                 fill = "navy") + 
  theme_minimal()
```

\newpage

Vemos que los residuos siguen aparentemente una distribución normal, como se mencionó antes, en la práctica, realizar una transformación de la variable dependiente con el fin de que siga una distribución normal suele ayudar en la distribución de los residuos [@walker2023a], sin embargo, la suposición de independencia de los residuos comúnmente se viola en los modelos que utilizan datos espaciales. Esto último por la autocorrelación espacial presente en el término de error, lo que significa que el rendimiento del modelo en sí mismo depende de la ubicación geográfica.

Podemos evaluar esto utilizando el índice de Moran, buscamos armar una matriz de pesos, para medir la interacción entre cada tramo censal, para ello generamos una "neighborhood list" en R con el paquete $\textbf{spdep}$ usando la función poly2nb(), esta función nos presenta varias formas de catalogar a los vecinos, para el modelo usaremos

-   $\textit{Contiguity-based neighbors} \rightarrow$ se usa cuando la geometría es tipo polygon, las opciones incluyen neighbors tipo "Queen", que se basa en que todos los polygons que compartan un vértice se consideren vecinos, y neighbors tipo "Rook", en donde deben compartir al menos un segmento de linea para ser vecinos, podemos inferir que neighbors tipo "Queen" abre diagonales de manera que tiene menos entradas iguales a cero

```{r, results='hide', warning=FALSE, message=FALSE}
# Crea neighbor list
nyc_nb <- spdep::poly2nb(nyc_data_prepped, queen = TRUE)
nyc_nb
```

```{r, echo = FALSE, fig.align = 'center', out.width="80%"}
knitr::include_graphics("image/neighborlist.png")
```

Interpretamos la información de la siguiente manera

-   Tenemos 1967 distritos censales en NYC (aquellos con valor NA se omiten)

-   El porcentaje de las entradas $w_{i,j}$ tales que $w_{i,j} \neq 0$ es $0.276$%

-   El número promedio de conexiones entre distritos censales, el promedio de cuantas conexiones posee cada distrito es $5.44$

-   Existen 5 distritos que no se conectan con nadie.

\newpage

Podemos hacer un plot de la estructura que posee el objeto "neighborhood list".

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}

# Filtramos para visualizar condado de Bronx
bronx <- nyc_data_prepped[str_detect(nyc_data_prepped$NAM, "Bronx"),]

# Guardamos su geometría (polygons)
bronx_geom <- st_geometry(bronx)

# Centroide de cada tramo censal y coordenadas
bronx_centroids = st_centroid(bronx_geom)
bronx_coordinates = st_coordinates(bronx_geom)

# Creamos neighbor list
bronx_nb_queen <- spdep::poly2nb(bronx)
bronx_nb_rook <- spdep::poly2nb(bronx, queen=FALSE)

```

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# save plot 
jpeg("image/bronx_neighborhood_rook_queen.jpg", width = 1000, height = 800)

# Preparamos plot 
par(mfrow = c(1,2))

# Plot Modelo Queen
plot(bronx_geom,
     main = "Queen",
     reset = FALSE,
     cex.main = 3)
plot(bronx_nb_queen, bronx_centroids,
     add = TRUE,
     col = 2,
     lwd = 1.5)

# Plot Modelo Rook
plot(bronx_geom,
     main = "Rook",
     reset = FALSE,
     cex.main = 3)
plot(bronx_nb_rook, bronx_centroids,
     add = TRUE,
     col = 2,
     lwd = 1.5)

# Cierra entorno plot
dev.off()
```

```{r, echo = FALSE, fig.align = 'center', out.width="90%"}
knitr::include_graphics("image/bronx_neighborhood_rook_queen.jpg")
```

En efecto, observamos la presencia de conexiones diagonales para el modelo "Queen" a diferencia de la estructur de "Rook", así como en ajedrez, la reina puede moverse en todas direcciones, mientras que la torre solo puede moverse en vertical y horizontal.

Dada la "neighborhood list" creada, se usa para crear una matriz de pesos $W$ usando la función de *spdep* nb2listw(), que nombramos *wts*, con ella se realiza un test de Índice de Moran.

```{r, results='hide', warning=FALSE, message=FALSE}
# Crea matriz de pesos estandarizada
library(spatialreg)
library(spdep)
set.ZeroPolicyOption(TRUE)
get.ZeroPolicyOption()
wts <- spdep::nb2listw(nyc_nb)
```

\newpage

Visualizando la matriz de pesos creada, guardado como *wts*

```{r, echo = FALSE, fig.align = 'center', out.width="70%"}
knitr::include_graphics("image/matrix.png")
```

Vemos que es una matriz cuyas filas tienen por suma total 1, por ende una matriz de pesos estandarizada, la cual para cada distrito, le da mismo peso a cada vecino que posea.

Con ella podemos realizar un test de índice de Moran, con el fin de verificar si existe autocorrelación espacial en los residuos del modelo, pues dado el caso, la muestra usada en el modelo no cumple ser distribuida identica e independientemente, explicando parte de la deficiencia del modelo de regresión lineal e implicando que el rendimiento del modelo dependa de la ubicación geográfica.

```{r, results = 'hide'}
# Realiza test de Moran 
spdep::moran.test(nyc_data_prepped$residuals, wts)
```

```{r, echo = FALSE, fig.align = 'center', out.width="75%"}
knitr::include_graphics("image/moranlm.png")
```

El valor esperado de $I$ bajo la hipótesis nula $H_0$ de no autocorrelación es de $-0.00051$, por lo que un valor de $I = 0.345$ es estadísticamente significativo,  el p-valor que toma el estadístico es $< 2.2e-16$ lo que nos habla de la significancia que posee en el modelo.

El test de índice de Moran concluye rechazar la hipótesis nula.

Por lo demás, vemos que el índice de Moran tiene un valor positivo, lo que nos dice que en el mapa, distritos censales con atributos similares tienden a agruparse (clusters).

\newpage

Mediante un *Moran scatterplot*, que consta de un gráfico de datos espaciales comparando los valores de una variable de respuesta $Y$ con sus valores 'lagged', que denotamos $Y_{lag} = WY$, el cual es una versión ponderada de las observaciones vecinas de cada punto, se usa función lag.listw(), junto a la matriz de pesos *wts* y los residuos del modelo.

```{r, echo=FALSE, fig.align = 'center', out.width="75%", warning=FALSE, message=FALSE}
# Obtiene version lagged
nyc_data_prepped$lagged_residuals <- 
lag.listw(wts, nyc_data_prepped$residuals)

# Plot
(morans_i_res <- nyc_data_prepped %>% 
ggplot(aes(x = residuals, y = lagged_residuals)) + 
  theme_minimal() + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", color = "red"))

# Guarda Plot
ggsave("image/morans_i_res.png",
       plot = morans_i_res)
```

El plot muestra en el eje horizontal los residuo del modelo, y en el eje vertical, la versión ponderada por la matriz de pesos de los residuos, en donde observamos una **correlacíon positiva**, esto significa que cuando el residuo de un distrito crece, entonces un promedio ponderado de sus alrededores tiende a crecer, en otras palabras, el error del modelo toma valores similares respecto de los valores que toma en sus alrededores, dando cuenta de la autocorrelación en los términos del error, lo que **sugiere rechazar la independencia en los residuos del modelo**.

Motivados por lo anterior, hacemos uso de modelos de regresión que toman en cuenta la autocorrelación espacial presente en los datos, considerando la matriz de pesos que da cuenta de la magnitud de la dependencia espacial.

-   Para los códigos utilizados en el modelado de imágenes, ver **Anexo 6**

-   Para profundizar en el uso de matrices de peso, ver **Capítulo 7** ['Analyzing US Census Data - Spatial analysis with US Census Data'](https://walker-data.com/census-r/spatial-analysis-with-us-census-data.html)

-   Para complementar en el análisis de datos, ver el **Capítulo 8** ['Analyzing US Census Data - Modeling US Census Data'](https://walker-data.com/census-r/modeling-us-census-data.html), capítulo donde se estudian conceptos durante la primera sección como indices de segregación y diversidad los cuales son usados en ciencias sociales para explicar patrones demográficos, en la segunda sección se estudian tópicos en modelamiento estadístico, incluyendo métodos de regresión con atributos espaciales, en donde tomamos en cuenta conceptos como la autocorrelación espacial que está inherente en la mayoría de las variables del censo; en la tercera sección del capítulo se estudian conceptos como clasificación, clusterización y regionalización, que son comunes en técnicas de Machine Learning.

\newpage

# Modelos de regresión espacial

Aplicamos cuatro modelos distintos de regresión espacial con los distintos paquetes que proporciona R.

Cada uno de ellos son adaptaciones de distintas formulaciones que consideran una matriz de pesos para hacer frente a los problemas asociados a data espacial.

Consideramos primero dos modelos autorregresivos, los cuales vienen incluidos en el paquete $\textbf{spatialreg}$ que se enfocan en añadir términos para abordar la autocorrelación espacial

## Spatial Lag Model

El modelo de *lag* espacial tiene en cuenta la dependencia espacial al incluir una variable de retraso sobre el resultado del modelo. Al hacerlo, se tienen en cuenta los efectos de dependencia espacial, es decir, la posibilidad de que los valores en áreas vecinas influyan en los valores en una ubicación determinada. Más precisamente, el modelo tiene la formulación:

$$ Y = \alpha + \rho WY + X \beta + \varepsilon$$
Donde formulamos el valor de la respuesta $Y$ como una versión ponderada de los valores que toma la respuesta en los alrededores de cada dato, sumado a una dependencia lineal que tenga la variable $Y$ con los regresores $X$, tomando $W$ una matriz de pesos estandarizada y $\rho$ refleja la intensidad de la dependencia espacial.

Para visualizarlo más claramente, vemos para cada fila tenemos 
$$Y_i = \alpha_i + \rho Y_{lag-i} + \sum_{k} \beta_k X_{ik} + \varepsilon_i \ \ \forall i \in \{1,...,n\}$$
donde 
$$Y_{lag-i} = \sum_{j}w_{ij} Y_j$$
Por lo que, la variable $Y_{lag-i}$ representa una versión ponderada de los valores que toman las respuestas $Y_j$ que son vecinas de la respuesta $Y_i$, es decir, aquellos $Y_j$ tales que $w_{ij} \neq 0$.

Podemos ver este modelo como una versión ponderada por por el término $(I- \rho W)^{-1}$ sobre el valor esperado $\alpha + X\beta$ y el residuo del modelo de una regresión lineal clásica [@8444678], basta notar que

$$Y = \alpha + \rho WY + X \beta + \varepsilon$$
$$\implies Y - \rho WY = \alpha + X \beta + \varepsilon$$
$$\implies (I-\rho W)Y = \alpha + X \beta + \varepsilon$$
$$\implies Y = \alpha' + (I- \rho W)^{-1} X \beta + (I- \rho W)^{-1}\varepsilon$$

Para estimar los parámetros de este modelo, podemos usar el método de máxima verosimilitud, usaremos en este caso funciones incluidas en el paquete *spatialreg*.

\newpage

Con el paquete $\textbf{spatialreg}$ hacemos uso de lagsarlm(), el formato para crear el modelo es similar a los usados en la regresión lineal, usamos como parámetro nuestra matriz de pesos *wts*.

```{r, results = 'hide'}
lag_model <- spatialreg::lagsarlm(
  formula = formula, 
  data = nyc_data_prepped, 
  listw = wts,
  zero.policy=TRUE #Importante  omitir valores NA
  )
summary(lag_model, Nagelkerke = TRUE)
```

```{r, echo = FALSE, fig.align = 'center', out.width="60%"}
knitr::include_graphics("image/lagsarlm.png")
```

Al igual que el modelo de regresión lineal se nos entregan los valores de $\alpha$ (intercept) y los $\beta_i$ asociados a cada regresor, junto a sus p-valores, lo que nos permite hacer un análisis de la significancia que tiene cada regresor en el modelo, por ejemplo, para este modelo vemos que la densidad poblacional no es un factor importante en el modelo, al igual que el porcentaje de extranjeros nacidos (pct_foreign_born), mientras que factores como el ingreso promedio (median_income) o el porcentaje de viviendas en el tramo censal que están siendo ocupadas por sus propietarios (percent_ooh) resultan ser cruciales en la predicción, respecto de esto útlimo, vemos que estas relaciones estadísticas se mantienen significativas en comparación al modelo de regresión lineal, mientras que otras como la cantidad de habitaciones promedio en el tramo censal (median_room) aumentan su p-valor.

\newpage

Además, el modelo calibra el valor de $\rho$, que refleja la intensidad de la matriz $W$, este toma un valor positivo y estadísticamente significativo, pues su p-valor es del orden de $2.22 \times 10^{-16}$, lo que sugiere la dependencia espacial, el modelo además entrega un test de autocorrelación en los residuos del modelo, en donde indíca un p-valor de $0.227$ que habla de la poca significancia que tiene el fenómeno en los residuos del modelo bajo esta nueva formulación.

El argumento 'Nagelkerke = TRUE' calcula un valor de pseudo-R cuadrado, que es mayor al valor correspondiente para el modelo de regresión lineal, con un valor de $0.59$.

Los valores de pseudo R-cuadrado no son directamente comparables al R-cuadrado de los modelos de mínimos cuadrados, tampoco se pueden interpretar como la proporción de la variabilidad en la variable dependiente que es explicada por el modelo, más bien, las medidas de pseudo R-cuadrado son medidas relativas entre modelos lineales similares que indican qué tan bien el modelo explica los datos.

## Spatial Error Model

En contraste con el modelo de Lag, los modelos de error espacial incluyen un *lag* en el término de error del modelo. Esto está diseñado para capturar procesos espaciales latentes que actualmente no se están teniendo en cuenta en la estimación del modelo y, a su vez, aparecen en los residuos del modelo [@walker2023a].

El modelo de error espacial se puede escribir de la siguiente manera:

$$ Y = \alpha + X \beta + u$$
Donde u es un término autorregresivo de la forma 
$$u = \lambda u_{lag} + \varepsilon$$
Y 
$$u_{lag} = Wu \implies u = \lambda Wu + \varepsilon$$
Es decir, para cada fila

$$ Y_i = \alpha + \sum_{k} \beta_{k}X_{ik} + \lambda u_{lag-i} + \varepsilon_i$$
Donde 
$$u_i = \lambda u_{lag-i} + \varepsilon_i \ , \ u_{lag-i} = \sum_{j} w_{ij} u_{j}$$
Por lo que buscamos calibrar $\lambda$ y $u$ con un modelo de la forma 
$$Y_i = \alpha + \sum_{k} \beta_{k}X_{ik} + \lambda \sum_{j} w_{ij} u_{j} + \varepsilon_i $$

Notemos que llegamos a una formulación similar al caso del modelo Lag, sin embargo buscamos calibrar $u$ que se incluye en el término del error al considerarlo de la forma $u = \lambda Wu + \varepsilon$.

\newpage

Para la calibración de parámetros, hacemos uso de la función errorsarlm de *spatialreg*

```{r, results='hide'}
error_model <- spatialreg::errorsarlm(
  formula = formula, 
  data = nyc_data_prepped, 
  listw = wts,
  zero.policy=TRUE
)
summary(error_model, Nagelkerke = TRUE)
```

```{r, echo = FALSE, fig.align = 'center', out.width="65%"}
knitr::include_graphics("image/errormodellm.png")
```

La calibración entrega un modelo cuyo pseudo-$R^2$ es de $0.589$, el cual es ligeramente menor al pseudo-$R^2$ del modelo Lag, por lo demás, *spatialreg* entrega los parámetros del modelo en el mismo formato.

Vemos que el valor que toma $\lambda$ es estadísticamente significativo, con una magnitud de $\lambda = 0.667$ y un bajo p-valor, esto da cuenta nuevamente de la importancia de considerar la autocorrelación espacial del modelo

Los modelos de lag espacial y de error espacial ofrecen enfoques alternativos para tener en cuenta los procesos de autocorrelación espacial al ajustar modelos.

Según Walker [@walker2023a], respecto de cual de los dos modelos usar, debe considerarse el contexto del tema en estudio, por ejemplo, si los efectos de dependencia espacial están relacionados con las hipótesis que el analista evalúa, como es el caso del efecto de los valores de las viviendas vecinas en torno al valor de una vivienda, se puede preferir un modelo Lag, por otro lado, si hay factores autocorrelacionados espacialmente y que probablemente influyen en la variable de respuesta $Y$ pero son difíciles de medir cuantitativamente, como puede ser la discriminación o el sesgo racial en el mercado de la vivienda, podría ser preferible un modelo de error espacial.

\newpage

## Test de Índice de Moran

Para complementar, podemos aplicar un test de índice de Moran sobre los residuos de ambos modelos de regresión espacial, para ver si resuelven el problema de la dependencia espacial de los errores.

**Spatial Lag Model**

```{r, include=FALSE,results = 'hide'}
# Moran Test Lag Model 
spdep::moran.test(lag_model$residuals, wts)
```

```{r, echo = FALSE, fig.align = 'center', out.width="80%"}
knitr::include_graphics("image/moran_lagmodel.png")
```

**Spatial Error Model**

```{r, include=FALSE,results='hide'}
# Moran Test Error Model
spdep::moran.test(error_model$residuals, wts)
```

```{r, echo = FALSE, fig.align = 'center', out.width="85%"}
knitr::include_graphics("image/error_moran.png")
```

Ambos modelos reducen el índice de Moran, acercandose a su valor esperando bajo la hipótesis nula $H_0$ de no autocorrelación, sin embargo, el modelo de error espacial hace un mejor trabajo sobre la autocorrelación espacial en los residuos, esto pues, si bien los modelos toman valores similares de $I$, el modelo de error espacial asigna un p-valor de $0.99$ al estadístico mientras que en el modelo Lag toma un p-valor de $0.7404$, eliminando por completo la dependencia espacial en el error.

\newpage

## Comparativa: Regresión Lineal, Lag Model, Error Model

Realizamos una comparativa entre el primer modelo de regresión lineal y los dos modelos de regresión espacial que hemos introducido, usando los paquetes $\textbf{jtools}$ y $\textbf{huxtable}$ por medio de la función export_summs().

```{r, include=FALSE, results='hide', message=FALSE, warning=FALSE}
library(jtools)
library(huxtable)
jtools::export_summs(model1, lag_model, error_model)
```

```{r, echo = FALSE, fig.align = 'center', out.width="70%"}
knitr::include_graphics("image/modelcomparing.png")
```

Dicha función nos permite exportar un resumen de los tres modelos, indicando los valores de los parámetros ya calibrados, la significancia estadística que estos toman usando el p-valor, además de una comparativa adaptada de $R^2$ con el fin de medir la calidad de la precisión que tienen los modelos, mostrando una medida de la varianza de la variable dependiente explicada por medio de los regresores.

\newpage

Respecto de la calidad de predicción según $R^2$, tenemos en orden de menor a mayor desempeño

-   Model 1: Regresión Lineal - lm() $\rightarrow R^2 = 0.46$

-   Model 2: Spatial Lag Model - spatialreg::lagsarlm() $\rightarrow R^2 = 0.63$

-   Model 3: Spatial Error Model - spatialreg::errorsarlm() $\rightarrow R^2 = 0.65$

Por lo que, ambos modelos de regresión espacial, logran una amplia mejoría respecto del modelo de regresión lineal simple.

Los modelos abordados en las secciones anteriores, tanto la regresión lineal como sus adaptaciones espaciales, estiman relaciones globales entre la variable dependiente y sus regresores $(\beta_i)_{i=1}^{m}$, es decir, a pesar de que estos modelos dan cuenta de la autocorrelación espacial, asumen que los coeficientes $\beta$ son iguales para toda ubicación, por lo tanto, no abordan la **heterogeneidad espacial**, ya que los datos que conforman un sector no siguen una distribución idéntica, por lo que es razonable suponer que la relación entre los regresores y la variable dependiente que se observa para toda la región, varíe significativamente entre sectores.

\newpage

## Geographically Weighted Regression (GWR)

El siguiente modelo, aborda la heterogeneidad espacial aprendiendo un conjunto de parámetros $\beta_{ik}$ asociado a los regresores del modelo en cada ubicación $s_i$, debido a esto, uno de los inconvenientes de este modelo es el coste computacional, pues para cada regresor, necesitamos entrenar $\beta_{ik}$ para ubicación $s_i$.

Mientras que la formulación de un modelo de regresión lineal, el cual tiene la forma 
$$Y_i = \alpha_i + \sum_{k=1}^{m} \beta_{k} X_{ik} + \varepsilon_i$$
Con los $\beta_k$ idénticos para cada locación $s_i$, la formulación del modelo de regresión geográficamente ponderado (GWR) para una ubicación $s_i$ dada, se escribe como 
$$Y_i = \alpha_i + \sum_{k=1}^{m} \beta_{ik} X_{ik} + \varepsilon_i$$ 
Donde el intercepto $\alpha_i$ , los regresores $(X_{ik})_{k=1}^{m}$, y los errores $\varepsilon_i$ están todos a locación $s_i$.

Por lo demás, los parámetros $\beta_{ik}$ serán coeficientes locales para el regresor $X_k$ con ubicación $s_i$

El coeficiente $\beta(s_h)$ en locación $s_h$ puede entrenarse vía método de mínimos cuadrados ponderados, donde el peso que se le asigna a cada muestra depende de su distancia a $s_h$, es decir, buscamos encontrar $\beta(s_h)$ tal que 
$$\beta(s_h) = \arg \min _{\beta(s_{h})} \sum_{i} w(s_i,s_h) (y(s_i) - x(s_i)^{T} \beta(s_h))^2$$
donde $y(s_i)$ y $x(s_i)$ son la respuesta y los regresores en locación $(s_i)$ respectivamente, además $w(s_i,s_h)$ es una función que decae con la distancia entre $s_i$ y $s_h$, por ejemplo, podemos tomar $w(s_i,s_h) = \exp (- \frac{1}{2} ||s_i-s_h||_{2}^{2})$ (Jiang, Z 2019) [@8444678]

Para $n$ datos, considerando la notación matricial antes usada, tomando $W=(w(s_i,s_h))_{i,h}^{n}$ como matriz de pesos, y manteniendo implícita la componente espacial, escribimos la calibración de $\beta_h$ vía mínimos cuadrados ponderados como $$\beta(s_h) = \arg \min _{\beta(s_{h})} \sum_{i} w_{ih} (Y_i - \sum _{i=1}^{n} X_{ik} \beta_{ih})^2$$ Podemos calibrar los parametros haciendo uso de los paquetes *GWmodel* y *spgwr*, el modelo se basa en el concepto de *kernel bandwidth* (ancho de banda de kernel) para computar un modelo de regresión local en cada ubicación, el cual se basa en un tipo de kernel, fijo o adaptativo, y asignandole una estructura a $w_{ij}$ que decaiga con la distancia.

Un kernel fijo utiliza una distancia de corte para determinar qué observaciones se incluirán en el modelo local para una ubicación dada , mientras que un kernel adaptativo utiliza los vecinos más cercanos a una ubicación dada [@walker2023a].

Para efectos del código, los tamaños de ancho de banda (un límite de distancia o el número de vecinos más cercanos) pueden ser seleccionados directamente por el usuario; en el paquete *GWmodel*, se nos proporciona la función bw.gwr() que ayuda a elegir un ancho de banda de kernel apropiado mediante validación cruzada.

Para las simulaciones, se hace uso de dicha herramienta, junto a un kernel adaptativo y para la función de pesos, se calcula una función de decaimiento 'bisquare' de la forma $$w_{ij} = 1 - (\frac{d_{ij}^2}{h^2})^2$$ donde $d_{ij}$ es la distancia entre las observaciones en locación $s_i$ y sus vecinos en locación $s_j$, como tomamos un kernel de tipo adaptativo, el valor de $h$ varía, y tomará la distancia entre la ubicación $s_i$ y el vecino más lejano a dicha ubicación.

Con el fin de comparar dos calibraciones del mismo modelo, haremos uso de los métodos *spgwr* el cual implementa un enfoque básico de GWR, para un ancho de banda fijo y un esquema de ponderación que viene dado mientras que *GWModel* optimiza dichos parámetros

**Modelo GWR: GWModel**

Para el método gwr.basic() del paquete GWmodel, debemos consiste convertir el conjunto de datos en un objeto SpatialPolygons sp (no admite objetos sf) y en seleccionar un "bandwidth" (ancho de banda), la estructura del código es la siguiente:

```{r, results='hide', message=FALSE}
# Convierte a Spatial Polygon 
nyc_data_prepped_sp <- nyc_data_prepped %>%
  as_Spatial()

# Optimiza bandwidth
bw <- bw.gwr(
  formula = formula, 
  data = nyc_data_prepped_sp, 
  kernel = "bisquare",
  adaptive = TRUE
  )
  
  # Entrena el modelo
  gw_model <- gwr.basic(
  formula = formula, 
  data = nyc_data_prepped_sp, 
  bw = bw,
  kernel = "bisquare",
  adaptive = TRUE
  )
  
  # Guarda resultados del modelo
  gw_model_results <- gw_model$SDF %>%
  st_as_sf() 
```

\newpage

El modelo nos entrega el siguiente resumen de los resultados

```{r, include =FALSE,results='hide', message=FALSE}
print(gw_model)
```

```{r, echo = FALSE, fig.align = 'center', out.width="90%"}
knitr::include_graphics("image/GWRprint.png")
```

La tabla *Summary of GWR coefficient estimates* entrega las estadísticas entre las que se mueven los parámetros $\beta_{ik}$, por otro lado, *Model calibration information* resume el tipo de modelo usado, el cual fue de kernel adaptativo con función de decaimiento *bisquare*, junto a la distancia euclidiana.

Además el modelo entrega una medida de $R^2 =0.721$, lo que es una mejoría notable en cuanto a la precisión del modelo, comparado con los modelos de lag y error, los cuales no consideran el problema de la **heterogeneidad espacial**

\newpage

Además, debido a que los coeficientes calibrados tienen asociados una posición geográfica, podemos observar una medida local del $R^2$ respecto del desempeño del modelo por distritos, y poder visualizar las zonas en donde predice mejor el modelo.

Primero tenemos que añadir los datos de R2 local al objeto sp, después de esto podemos utilizar el paquete *tmap* para crear un mapa para el modelo *GWRmodel*.

```{r}
# Añadimos la data como objeto sp
nyc_data_prepped_sp$gwmodel <- gw_model$SDF$Local_R2

# Print
nyc_gw_model <- tm_shape(nyc_data_prepped_sp) +
  tm_fill("gwmodel", 
          palette = "RdBu", 
          title = "Local R2") +
  tm_layout(main.title = "GWR [GWModel] Local R2")
nyc_gw_model
```

\newpage

**Modelo GWR: GWModel**

Hacemos una comparativa con el modelo calibrado por el paquete *spgwr*, la estructura es la siguiente

```{r, results='hide', message=FALSE}
# Optimiza bandwitch
bw2 <- gwr.sel(formula = formula, 
               data = nyc_data_prepped_sp,
               adapt = TRUE,
               gweight = gwr.bisquare,
               method = "cv",
               verbose = TRUE)
               
# Entrena modelo               
spgwr_model <- gwr(formula = formula, 
            data = nyc_data_prepped_sp,
            adapt = bw2,
            gweight = gwr.bisquare, 
            hatmatrix = TRUE)
```

Al ejecutar el código, se nos entrega la siguiente tabla resumen del modelo

```{r, echo = FALSE, fig.align = 'center', out.width="70%"}
knitr::include_graphics("image/spgwrResume.png")
```

Al igual que con el método *GWModel*, nos entregan estadísticas respecto de los valores que toman los regresores para las distintas locaciones acompañado de otros coeficientes, como $R^2$, la suma del cuadrado de los residuos, entre otros parámetros asociados.

\newpage

El modelo arroja un valor 'cuasi-global' de $R^2 = 0.72$, lo que es imperceptiblemente menor, por lo que procedemos a comparar el desempeño de los modelos respecto de su $R^2$ local

```{r, echo =FALSE, message=FALSE, fig.align = 'center', out.width="80%"}
# Añadimos la cualidad a la data sp
nyc_data_prepped_sp$spgwr <- spgwr_model$SDF$localR2

# Print
nyc_spgwr <- tm_shape(nyc_data_prepped_sp) +
  tm_fill("spgwr", 
          palette = "RdBu", 
          title = "Local R2") +
  tm_layout(main.title = "GWR [spgwr] Local R2")

tmap_arrange(nyc_gw_model, nyc_spgwr)
```

Notamos que el modelo calibrado por el paquete GWModel predice mejor a nivel general, y existen zonas en donde el método *spgwr* posee un notorio peor desempeño en comparación al método *GWModel*

\newpage

## Geographically Weighted Random Forest

Finalmente, a modo experimental, implementamos el siguiente modelo que aborda la heterogeneidad espacial mediante un conjunto de modelos de bosques aleatorios (RF) localmente calibrados,  los modelos RF localmente desarrollados solo usan una cierta cantidad de puntos de datos vecinos para entrenar el modelo  [@georganos2022].

La ecuación para la calibración se formula como
$$Y_i = a(s_i)x(s_i) + \varepsilon$$
Donde $s_i$ referencia la posición geográfica y $x(s_i)$ son los datos de entrenamiento

La diferencia entre un GWR, con una formulación lineal y un GRF, es que podemos modelar la no-estacionariedad de los datos junto con un modelo no lineal, lo que vuelve menos restríctiva la función que queremos modelar.

Para entrenar el modelo hacemos uso del paquete *SpatialML* para aplicar el modelo de bosque aleatorio. Este modelo necesita que se le pase como argumento un objeto de coordenadas, por lo que trabajamos con los centroides de las geometrías.

Este modelo es diferente del clásico random forest, ya que se construye un submodelo para cada ubicación de datos teniendo en cuenta sólo las observaciones cercanas. Estas observaciones cercanas se deciden por la selección del "bandwidth" óptimo. Sin embargo, este proceso consume mucho tiempo, por lo que se escoge aleatoriamente un valor de 98 en bandwidth para no sobrecargar el coste computacional.

Al entrenar el modelo, el paquete entrega el siguiente resumen.

```{r, include=FALSE, results='hide', message=FALSE}
# Paquetes
library(SpatialML)
library(sf)
library(tidyverse)

# Obtenemos centroides
nyc_data_prepped <- nyc_data_prepped %>% 
  mutate(lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),
         lat = map_dbl(geometry, ~st_centroid(.x)[[2]]))

# Creamos columna de coordenadas
coords <- nyc_data_prepped %>% 
  st_drop_geometry() %>% 
  select(lat,lon)

# Filtramos
nyc_grf_prepped <- nyc_data_prepped %>% 
  st_drop_geometry() %>%
  mutate(sqrt_med_value = sqrt(median_value)) %>% 
  select(!c(GEOID, NAM, residuals, lagged_residuals, lat, lon))

# Definimos formula para GRF
formula_grf <- "sqrt_med_value ~ median_rooms + median_income + 
pct_college + pct_foreign_born + pct_white + pct_black + 
pct_hispanic + pct_asian + median_age + percent_ooh  + 
pop_density"


# Optimizamos bandwitch
bwgrf <- grf.bw(formula = formula_grf,
                dataset = nyc_grf_prepped,
                kernel = "adaptive",
                coords = coords,
                bw.min = 98,
                bw.max = 98,
                step = 1,
                trees = 500,
                mtry = NULL,
                importance = "impurity",
                forests = FALSE,
                weighted = TRUE,
                verbose = TRUE)

# Entrena el modelo
grf_model <- grf(formula = formula_grf,
    dframe = nyc_grf_prepped,
    bw= 98,
    ntree = 500,
    mtry = 2,
    kernel = "adaptive",
    forests = TRUE,
    coords = coords)
```

```{r, echo = FALSE, fig.align = 'center', out.width="80%"}
knitr::include_graphics("image/randomforestresult.png")
```

Entregando una medida de $R^2 = 0.551$, que es notoriamente inferior a los modelos clásicos de regresión geográficamente ponderada, sin embargo, sigue entregando mejores resultados que una regresión lineal simple en cuanto a $R^2$ global.

Para ver el código utilizado en la implementación del modelo, ver **Anexo 7**

\newpage

## Comparativa: R2 Local en modelos GWR y GRF

El modelo GRF trae consigo una medida de $R^2$ local, por lo que procedemos a comparar gráficamente con respecto del modelo GWR

```{r, message=FALSE, results='hide'}
# Añadimos R2 local 
nyc_data_prepped_sp$grf <- grf_model$LGofFit$LM_Rsq100 #R2 Local
```

```{r, include=FALSE, message=FALSE, results='hide'}
# Print
nyc_grf <- tm_shape(nyc_data_prepped_sp) +
  tm_fill("grf", 
          palette = "RdBu", 
          title = "Local r2") +
  tm_layout(main.title = "GRF [SpatialML] Local R2")
```


```{r, echo = FALSE, fig.align = 'center', out.width="100%"}
knitr::include_graphics("image/comparativagwmodel.png")
```

Podemos ver que el modelo *GWR* del paquete *GWmodel* tiene, con diferencia, las puntuaciones $R^2$ locales más altas, seguidas por el paquete *spgwr* (GWR) y finalmente *SpatialML* (GRF)

También es importante considerar el coste computacional de los modelos, para el caso del modelo GRF, los tiempos de ejecución durante el entrenamiento de los parámetros es notablemente mayor que el del modelo GWR, además, la memoria consumida para almacenar datos en el caso de los métodos *GWModel* y *spgwr* no superan los 40MB, mientras que el modelo *GRF* calibrado por *SpatialML* requiere de 2.3GB

Por lo que, para efectos de los datos usados, el mejor método de predicción para el valor de las viviendas es el modelo GWR, ejecutado por la librería *GWModel*

Para los códigos usados en la visualización de datos, ver **Anexo 7**.

\newpage

# Conclusión

A lo largo del documento, se estudian las problemáticas asociadas al análisis de datos georreferenciados, tales como la autocorrelación espacial y la heterogeneidad espacial inmersa en los datos debido a su naturaleza demográfica.

Se presentan métodos para cuantificar la dependencia espacial, y realizar inferencia estadística en cuanto a este fenómeno, como es el caso del índice de Moran, y se realizan simulaciones con datos de la *American Community Survey (2016-2020)* para ver sus efectos sobre métodos de predicción clásicos, tales como la regresión lineal simple, modelo que no toma en cuenta las componentes espaciales de los datos, las cuales, llevan a rechazar las hipótesis asociadas a este, como es el caso de la independencia entre los residuos del modelo, esto no permite asegurar la calidad de la estimación de los parámetros $\beta$, pues dado el Teorema de Gauss-Markov, se necesitan de todas las hipótesis para asegurar que $\beta$ es el mejor estimador lineal insesgado, es por esta razón que se necesitan modelos que tomen en cuenta la dependencia espacial de los datos, con el fin de obtener una mejor predicción, a partir de esto, se realizan distintos modelos adaptados para la predicción espacial.

Los modelos de Lag y Error, los cuales toman en cuenta la magnitud de la dependencia espacial entre los datos, mediante una matriz de pesos estandarizada, abordan este problema, y logran mejores puntuaciones $R^2$ que el modelo de regresión lineal simple, donde en particular, el modelo de Error logra eliminar completamente la autocorrelación espacial en los residuos del modelo, sin embargo, sigue presente el problema de la heterogeneidad espacial, motivados por esto, introducimos  modelos de regresión geográficamente ponderados (GWR), los cuales ajustan un modelo lineal a cada ubicación de los datos, alcanzando valores $R^2$ globales más altos que los modelos de Lag y Error, esto pues al calibrar los parámetros para cada ubicación, somos capaces de medir las variaciones locales dentro del modelo, logrando tratar el problema de la heterogeneidad espacial, pues de esta manera, la relación lineal que tienen los regresores con la variable dependiente, no es global, sino que varía dependiendo de la ubicación.

Además, de forma experimental, se aplica un modelo basado en técnicas de Machine Learning, mediante Random Forest (GRF), sin embargo, obtuvo una puntuación menor términos de $R^2$ respecto de los modelos GWR, y se mantuvo cerca en cuanto a precisión con los modelos de Lag y Error.

Comparativamente, se nota una mejoría con respecto a la precisión de modelos de regresión clásicos. Estos resultados pueden ser útiles para diversos ámbitos de la industria, en cuyas tomas de decisiones intervengan datos que están asociados a una componente geográfica, como es el caso de la predicción de precios de vivienda dentro del mercado inmobiliario.

\newpage

# Referencias

::: {#refs}
:::

\newpage

# Anexo

## Anexo 1: Librerías

Las librerías durante la realización del proyecto son las siguientes:

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidycensus)
library(tidyr)
library(tidyverse)
library(censusapi)
library(tmap)
library(ggplot2)
library(dplyr)
library(stringr)
library(units)
library(stats)
library(grDevices)
library(dotenv)
library(sf)
library(corrr)
library(spatialreg)
library(spdep)
library(jtools)
library(huxtable)
library(GWmodel)
library(spgwr)
library(SpatialML)
library(ggthemes)
```

Para más información respecto de las librerías, consultar [Analyzing US Census Data - Kyle Walker](https://walker-data.com/census-r/)

\newpage

## Anexo 2: Activacion API key

Para hacer uso de la data que nos proporciona el paquete tidycensus debemos hacer uso de una *key* que nos permita el acceso a la información.

El siguiente código ejecuta la activación de la llave 'API Key' que nos permite descargar Census Data, mediante funciones como get_acs, el primer argumento es la llave utilizada en este código.

```{r, eval=FALSE}
census_api_key("6034739b488f5fc230e467601ed20256bb25831b", install = TRUE)
```

Este comando tiene la estructura

```{r, eval=FALSE}
census_api_key(key, overwrite = BOOL, install = BOOL)}
```

**Argumentos**

-   key: La API Key entregada por el Censo, ingresar con "". Se obtiene en [API Census](https://api.census.gov/data/key_signup.html).

-   overwrite: Si está en TRUE, sobreescribirá sobre una ya existente CENSUS_API_KEY que tengamos instalado en nuestro archivo .Renviron

-   install: Si está en TRUE, instalará la llave en nuestro archivo .Renviron para las futuras sesiones, de no existir crea uno. Viene en FALSE por defecto.

Despues de instalada la llave, puede usarse en cualquier momento llamando el siguiente comando

```{r}
Sys.getenv("CENSUS_API_KEY")
```

Reload del enviroment para poder usar la llave sin tener que resetear R

```{r, eval=FALSE}
readRenviron("~/.Renviron")
```

\newpage

## Anexo 3: Uso de tidycensus para la obtención y manejo de datos

Para obtener datos de las distintas bases *tidycensus* ofrece las siguientes funciones

-   get_decennial() : Solicita datos de las API US Decennial Census para 2000, 2010 y 2020.

-   get_acs() : Solicita datos de las muestras de la American Community Survey de 1 y 5 años. Los datos están disponibles desde el ACS de 1 año hasta 2005 y el ACS de 5 años hasta 2005-2009.

-   get_estimates() : Interfaz para las Population Estimates APIs. Estos conjuntos de datos incluyen estimaciones anuales de las características de la población por estado, condado y área metropolitana, junto con componentes de estimaciones demográficas de cambio como nacimientos, muertes y tasas de migración.

-   get_pums() : Accede a los datos de ACS Public Use Microdata Sample APIs, Estas muestras incluyen registros anónimos a nivel individual de la ACS organizados por hogar y son muy útiles para muchos análisis de ciencias sociales, get_pums() se cubre con más profundidad en los Capítulos 9 y 10 de *Analyzing US Census data*.

-   get_flows() : Interfaz para la ACS Migration Flows APIs. Incluye información sobre los flujos de entrada y salida de varias geografías para las muestras de ACS de 5 años, lo que permite realizar análisis de origen y destino

El código utilizado para elegir las variables necesarias para los modelos de predicción es el siguiente:

```{r, results='hide', warning=FALSE, message=FALSE}
#package
library(dotenv)
library(sf)
library(tidycensus)
library(dplyr)

# Variable: Condados que forman NYC
nyc_counties <- c("Bronx","Kings","New York","Queens","Richmond")

# Lista de regresores a utilizar
variables_to_get <- c(
  median_value = "B25077_001",
  median_rooms = "B25018_001",
  median_income = "DP03_0062",
  total_population = "B01003_001",
  median_age = "B01002_001",
  pct_college = "DP02_0068P",
  pct_foreign_born = "DP02_0094P",
  pct_white = "DP05_0077P",
  pct_black = "DP05_0078P",
  pct_hispanic = "DP05_0070P",
  pct_asian = "DP05_0080P",
  percent_ooh = "DP04_0046P"
)
```

\newpage

Para ver que variables se pueden obtener, $\textit{tidycensus}$ nos provee de la función load_variables(), dicha función requiere de 2 argumentos, $\textit{year}$ que toma el año de referencia de la data, y $\textit{dataset}$.

Para el Decennial Census 2000 a 2010, usar "sf1" o "sf2", el 2020 Decennial Census tambien acepta "sf3" y "sf4", sf hace referencia a Summary Files.

Para variables de la American Community Survey, debemos especificar el año de la encuesta, por ejemplo "acs1" para el primer año de la ACS, por ejemplo si se quiere acceder a la data $\textit{5-year ACS}$

```{r, results='hide', warning=FALSE, message=FALSE}
load_acs = load_variables(year = 2020, dataset = "acs5")
```

```{r, echo = FALSE, fig.align = 'center', out.width="80%"}
# Coeficientes del modelo
knitr::include_graphics("image/loadacs.png")
```

Más detalles en el capítulo 2 del libro [Analyzing US Census Data](https://walker-data.com/census-r/an-introduction-to-tidycensus.html)

\newpage

## Anexo 4: Uso de librerías ggplot2 y tmap para plots con referencias geográficas

Dada la naturaleza del problema de predicción georeferenciada, nos interesa visualizar la data junto a su componente espacial, a continuación se presenta un breve manual de uso de las librerías $\textit{ggplot2}$ y $\textit{tmap}$ para visualizar los datos acompañados de sus referencias geográficas, los cuales en R se codifican como objetos tipo \textit{polygon}, usando un ejemplo con datos provenientes de NYC.

### NY US Census data

Tomamos como caso de prueba al estado de New York, para visualizar el valor medio de las viviendas a nivel de condados, podemos variar el nivel geográfico con el parámetro $\textit{geography}$ (Walker K., 2023) [@walker2023a]

```{r, results='hide', warning=FALSE, message=FALSE}
# Creamos el objeto median_nyc que contiene la variable "median income"
# con nivel geografico condados(county).
medianincomenystate <- get_acs(geography = "county",
        state = "New York",
        geometry = TRUE, #descarga el componente espacial del tramo censal
        variable = "B19013_001" #median income 
        )
```

### Plot del estado New York

Comencemos a visualizar la información, probaremos primeramente con plot, y luego usaremos herramientas mas avanzadas que nos ofrecen los paquetes

```{r, fig.align = 'center', out.width='60%'}
plot(medianincomenystate["NAME"])
```

\newpage

### Plot del valor medio de las viviendas en New York, map-making con ggplot2 y geom_sf

En $\textit{ggplot2}$ podemos plotear rapidamente objetos de tipo $\textit{sf}$ mediante geom_sf(), para entender la sintaxis realizamos el siguiente plot de el estimado de la variable "Median income New York".

```{r, fig.align = 'center', out.width='50%'}
ggplot(data = medianincomenystate, aes(fill = estimate)) + 
  geom_sf()
```

Podemos customizar nuestros plots en ggplot2, la estructura es la siguiente

```{r, fig.align = 'center', out.width='60%'}
ggplot(data = medianincomenystate, aes(fill = estimate)) +
  geom_sf() +
  scale_fill_distiller(palette = "YlGnBu",
                       direction = 1) +
  labs(title = "Median income New York, 2016-2020",
       caption = "Data source: 2016-2020, US Census",
       fill = "Median income estimate") +
  theme_void()
```

\newpage

Las funciones que acompañan la sintaxis nos permiten customizar nuestro plot en ggplot2.

-   scale_fill_distiller() : Nos permite especificar una paleta de colores de [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3) en el plot.

-   labs() : Nos permite añadir título, caption, legend label en el plot.

-   theme_void() : Nos permite remover el fondo y la grilla cuadrícular.

### Histograma del valor medio de las viviendas en el estado New York

```{r, out.width='60%', fig.align = 'center'}
hist(medianincomenystate$estimate)
```

\newpage

### Map-making con tmap

La sintaxis en similar a la usada en $\textit{ggplot2}$, el objeto mapa se inicializa con la función tm_shape() y nos permite visualizar los distritos censales con tm_polygons()

```{r, out.width='60%', fig.align = 'center'}
library(tmap)
tm_shape(medianincomenystate) + 
  tm_polygons()
```

### Variables en tmap

Veamos nuevamente la variable "median income", para ello llamamos la variable a visualizar dentro de tm_polygons

```{r, out.width='60%', fig.align = 'center'}
library(tmap)
tm_shape(medianincomenystate) + 
  tm_polygons(col = "estimate")
```

\newpage

### Labels y otras opciones de diseño

Podemos añadir más variables y seguir personalizando nuestros plots, por ejemplo añadiendo histogramas por distintos tipos de clasificación, quantiles ("quantile"), equal intervals ("equal") y Jenks natural breaks ("jenks"), con tm_layout() nos permite customizar el estilo del mapa, del histograma y añadir leyendas.

```{r, out.width='85%', fig.align = 'center'}
tm_shape(medianincomenystate) + 
  tm_polygons(col = "estimate",
              style = "jenks",
              n = 5, #num de intervalos
              palette = "Purples",
              title = "2016 - 2020 US Census",
              legend.hist = TRUE) +
  tm_layout(title = "NY State Median Income", 
            frame = FALSE,
            legend.outside = TRUE,
            bg.color = "grey80", #backgroundcolor
            legend.hist.width = 7)
```

\newpage

### NYC Median Income

Podemos trabajar con niveles geográficos de menor nivel, como condados y tracts, trabajamos la ciudad de New York, formado por ciertos condados.

```{r, results='hide', warning=FALSE, message=FALSE}
# Creamos el objeto medianincomenyc que contiene la variable "median income"
# con nivel geografico de tract.
medianincomenyc <- get_acs(geography = "tract",
        state = "New York",
        geometry = TRUE, #descarga el componente espacial del tramo censal
        variable = "B19013_001" #median income 
        )
```

Filtramos respecto de los condados que queremos visualizar, aquellos cerca de la ciudad de NY, usamos $\textit{tidyr}$ con la función separate(), de manera de poder filtrar los condados que nos interesan

El siguiente código nos permite crear nuevas columnas "tract" y "county", de manera que podemos filtra aquellos condados de interes con otras funciones, ademas usamos na.omit() para botar aquellos valores con NA y así limpiar la data

```{r, results='hide', warning=FALSE, message=FALSE}
medianincomenyc <- separate(medianincomenyc,
         NAME,
         into = c("tract", "county"),
         sep = ", ")

medianincomenyc <- medianincomenyc %>% filter(grepl('Bronx County|New York County|Queens County|Kings County|Richmond County', county))

medianincomenyc <- na.omit(medianincomenyc)
```

\newpage

### Plot con tmap de NYC sobre los ingresos promedios

```{r, out.width= '90%', fig.align = 'center'}
tm_shape(medianincomenyc) + 
  tm_polygons(col = "estimate",
              style = "equal",
              palette = "Purples",
              title = "2016 - 2020 US Census",
              legend.hist = TRUE) +
  tm_layout(title = "NYC Median Income by Census Tract", 
            frame = FALSE,
            legend.outside = TRUE,
            bg.color = "grey80", #backgroundcolor
            legend.hist.width = 7)
```

Más detalles en el capítulo 6 de Analyzing US Census Data. Ver también [Tmap Book](https://r-tmap.github.io/tmap-book/index.html)

\newpage

## Anexo 5: Discusión sobre el índice de Moran

Veamos que, bajo ciertas condiciones de la matriz $W$, se tiene que $I \in [-1,1]$, en (Chen, Y. 2022)[@chen2022] tenemos una demostración de que el índice de Moran toma dichos valores cuando $W$ clasifica como "globally normalized wight matrix", pero no entra en detalles, además, se estudia que en la práctica muy típicamente para estas matrices, $I \in (-1,1)$, en el [manual de PQStat](https://manuals.pqstat.pl/en:przestrzenpl:mwagpl) se habla de "Spatial weights matrix" como una matriz con coeficientes positivos y filas estandarizadas de manera las filas sumen uno, esto es $\sum_{j=1}^{n} w_{ij} = 1 \ \ \forall i \in \{1,\dots,n\}$ ($\textit{row-normalized}$).

En la discusión ["Why is Moran's I coming out greater than 1" - StackExchange](https://stats.stackexchange.com/questions/160459/why-is-morans-i-coming-out-greater-than-1) se muestra un contraejemplo de matriz $W$ cuya suma de las entradas es $\sum_{i,j}^{n} w_{ij} = 1$ y para muestras $(X_1,\dots,X_4)$, donde sucede que $I = 3$, por lo que si $W$ no cumple la propiedad de ser $row-normalize$, no se puede saber a primeras los valores que puede tomar $I$, y por tanto, no tenemos una referencia clara de la significancia que pueda tener la magnitud de $I$ en el modelo si no conocemos sus valores máximos y mínimos.

Veremos que se puede relajar la condición de que la suma de las filas sean todas iguales a $1$ y pediremos que la suma de las filas sea un número $\alpha$ para cada fila, trabajaremos entonces con matrices de pesos estandarizadas que son las adecuadas para un modelo de autocorrelación

Presentamos una demostración en la que no usaremos técnicas de cálculo ni de optimización para probar que para $W$ una matriz de pesos estandarizada, tenemos que $I \in [-1,1]$

Haremos uso de los siguientes resultados conocidos

-   $\textbf{Teorema:}$ Sea $A$ una matriz cuadrada simétrica a coeficientes reales, tenemos que $A$ es ortogonalmente diagonalizable, es decir, podemos escribir $A = PDP^{T}$, con $P$ matriz ortogonal cuyos vectores columna son los vectores propios de $A$ y $D$ es matriz diagonal con $(d_{ii})_{i=1}^{n} = (\lambda_{i})_{i=1}^{n}$ valores propios de $A$

-   $\textbf{Lema:}$ Si $P$ es ortogonal, entonces $P^T = P^{-1}$ y además, $P$ es una isometría en $(\mathbb{R}^{n} , || \cdot ||_{2})$, es decir, $||Pw||_{2} = ||w||_{2}$ $\ \ \forall w \in \mathbb{R}^{n}$

Con ello, podemos probar el siguiente teorema

-   $\textbf{Teorema:}$ Se tiene que el operador $\lambda_{max}: S^n \rightarrow \mathbb{R}$ , que asocia a cada $A \in S^n$ matriz simétrica su mayor valor propio $\lambda_{max} (A)$, es convexa, más aún, $\lambda_{max}(A) = \sup_{||v||=1} v^{T}Av$ supremo de funcionales lineales sobre $S^n$.

El resultado anterior es conocido es corolario de los cocientes de Rayleigh, para una demostración detallada de ver Teorema 55 en [The Rayleigh's principle and the minimax principle for the eigenvalues of a self-adjoint matrix](https://people.math.osu.edu/costin.10/5102/Rayleigh%20quotient.pdf)

\newpage

Además haremos uso del siguiente lema

-   $\textbf{Lema:}$ El mayor valor propio de una matriz $A$ con coeficientes no negativos está acotada por la mayor suma de las filas, esto es, $\lambda_{max} (A) \leq \max_{i} \sum_{j=1}^{n}a_{ij}$.

-   $\textbf{Demostración del lema}$: Sea $\lambda$ un valor propio de una matriz $A$ no negativa asociada a un vector $x$, tenemos entonces que

$$Ax=\lambda x \implies \lambda |x_i| = |\sum_{j=1}^{n}a_{ij}x_j| \ \ \forall i \in \{1,\dots,n\}$$ $$\implies \lambda |x_i| = |\sum_{j=1}^{n}a_{ij}x_j| \leq (\sum_{j=1}^{n} |a_{ij}|) \max_{j} |x_j| \ \ \forall i \in \{1,\dots,n\}$$

Usando que $a_{ij} \geq 0 \ \ \forall i,j \in \{1,\dots,n\}$ y que la desigualdad se cumple para todo $i$ tenemos

$$\lambda \max_{i} |x_i| \leq \max_{i} (\sum_{j=1}^{n} a_{ij} \max_j |x_j|) = \max_{i}(\sum_{j=1}^{n} a_{ij}) \max_{j} |x_j|$$

$$\implies \lambda \leq \max_{i} (\sum_{j=1}^{n} a_{ij})$$

Como $\lambda$ era arbitrario deducimos que

$$\lambda_{max}(A) \leq \max_{i}(\sum_{j=1}^{n} a_{ij})$$

\newpage

Enunciamos entonces

-   $\textbf{Propiedad:}$ Para $I= \frac{n}{W}\frac{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} (y_i - \bar{y})(y_j - \bar{y})}{\sum _{i=1}^{n} (y_i - \bar{y})^2}$ donde $\bar{y} = \sum_{i=1}^{n} y_i/n$ con $W$ matriz de pesos estandarizada e $y \in \mathbb{R}^{n}$, se cumple que $I \in [-1,1]$.

-   $\textbf{Demostración:}$ Consideremos el operador $\Phi : \mathbb{R}^{n} \rightarrow \partial B(0,1)$ definido por $\Phi(y) = z =(z_i)_{i=1}^{n} = (\frac{(y_i - \bar{y})}{\sqrt(\sum_{i=1}^{n}(y_i - \bar{y})^{2})})_{i=1}^{n}$

Está bien definida pues $\forall y \in \mathbb{R}^n$

$$||\Phi(y)||_{2} = ||z||_{2} = \sum_{i=1}^{n} |z_i|^2 = \sum_{i=1}^{n} \frac{(y_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

$$= \frac{\sum_{i=1}^{n} (y_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = 1$$

Notamos que

$$I = \frac{n\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij}(y_i - \bar{y})(y_j - \bar{y}))}{W (\sum_{i=1}^{n} (y_i - \bar{y})^2)}$$

$$= \frac{n}{W} \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} \frac{(y_i - \bar{y})(y_j - \bar{y})}{\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}$$

$$\frac{n}{W} \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} \Phi (y)_i \Phi (y)_j = \frac{n}{W} \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} z_i z_j$$

Definimos $V = (\frac{w_{ij}}{W})_{i,j}^{n}$ matriz normalizada tal que $\sum_{i,j} v_{ij} = 1$, además, como W es matriz de pesos estandarizada, tenemos que $V$ también lo es, por lo que es simétrica, no negativa, y row-normalized.

Entonces, recordando que $||z||_{2} = 1$

$$I = n(z^{T}Vz) \leq n \sup_{||z||=1} z^{T}Vz = n\lambda_{max}(V)$$

$$\implies |I| \leq n |\lambda_{max}(V)| \leq n |\max_{i}(\sum_{j=1}^{n} v_{ij})| \leq n \max_{i} (\sum_{j=1}^{n}|v_{ij}|)$$

Por un lado, tenemos que $|v_{ij}| = v_{ij}$ y como $\sum_{i=1}^{n} \sum_{j=1}^{n} v_{ij} = 1$ y $V$ es $\textit{row-normalized}$ tenemos que se cumple $\sum_{j=1}^{n} v_{ij} = \frac{1}{n} \ \ \forall i \in \{1,\dots,n\}$, y por lo tanto

$$\implies |I| \leq n\max_{i}(\sum_{j=1}^{n} v_{ij}) = n *\frac{1}{n} = 1$$

$$\implies I \in [-1,1]$$

\newpage

En caso de que $W$ no cumpla la propiedad $\textit{row-normalized}$ de igual manera podemos encontrar una cota para $I$ y esta es $I \in [-n,n]$ donde $n$ es la cantidad total de datos haciendo uso de la desigualdad de Schur [@ikramov1994] que nos dice

-   $\textbf{Teorema:}$ Sea $A$ una matriz $n \times n$ con valores propios $(\lambda_1,\dots,\lambda_n)$, entonces se cumple

$$\sum_{i=1}^{n} |\lambda_i|^{p} \leq \sum_{i,j = 1}^{n} |a_{ij}|^{p} \ \ , 1 \leq p <2$$

Luego, como vimos antes, tenemos ahora $V$ una matriz normalizada, pero no necesariamente $\textit{row-normalized}$, luego

$$I = n (z^{T}Vz)$$ Y como $V$ es simétrica, la escribimos como $V=P^TDP$ usando el teorema espectral, y como $P$ es ortogonal, entonces $x = Pz$ tiene norma $||Pz||_{2} = ||x||_{2} = 1$, en particular $|x_i|^2 \leq 1 \ \ \forall i \in \{1,\dots,n\}$ luego

$$I = n(z^TP^T D Pz) = n(x^TDx) = n \sum_{i=1}^{n} \lambda_{i}(V) |x_{i}|^2$$ 
$$\implies |I| \leq n \sum_{i=1}^{n} |\lambda_i (V)| \leq n \sum_{{i,j = 1}}^{n} |v_{ij}| = n $$ 
$$\implies I \in [-n,n]$$
Retomando el ejemplo visto en ["Why is Moran's I coming out greater than 1" - StackExchange](https://stats.stackexchange.com/questions/160459/why-is-morans-i-coming-out-greater-than-1) donde $I=3$, vemos que $n = 4$ y entonces $I \in [-n,n]$.

Apesar de este resultado, usamos matrices de peso estandarizadas pues son las que mejor se ajustan al modelo, y donde sabemos que el índice de Moran se mueve entre $[-1,1]$.

Si sabemos el intervalo en el que se mueve $I$ para nuestro modelo, podemos saber que tan significativo es el valor, y por lo tanto que tan presente esta el fenómeno de autocorrelación espacial.

Respecto del valor esperado del índice de Moran bajo cuando suponemos que no hay autocorrelación espacial, consideramos la siguiente hipótesis nula: $$H_0: \text{No existe autocorrelación espacial}$$ Dicha hipótesis consiste en suponer que, dada una permutación de los datos, es decir, cambiando el orden en el que vienen los datos $(x_i)_{i=1}^{N}$, y fijando la matriz de pesos antes del reordenamiento, no se debiese tener un impacto sobre el valor de $I$.

\newpage

Por lo tanto, para un muestreo $x = (x_i)_{i=1}^{N}$, tomamos una permutación $\pi \in S^{N}$ que aplica sobre el conjunto $(x_i)_{i=1}^{N}$ con $S^N$ el grupo simétrico.

Consideramos nuestro espacio de probabilidad $(\Omega, \mathcal{F}, \mathbb{P}) = (S^N, \mathcal{P}(S^N), \mathbb{P}_u)$ con $\mathbb{P}_u$ distribución uniforme.

Tenemos entonces

$$\mathbb{E}_{\pi}(I) = \frac{N}{W} \frac{\sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} ((\pi x)_i - \pi \bar{x})((\pi x)_j - \pi \bar{x})}{\sum _{i=1}^{N} ((\pi x)_i - \pi \bar{x})^2}$$

Claramente, $\pi \bar{x} = \bar{x}$, además haciendo la suma sobre $i$ tenemos

$$\sum _{i=1}^{N} ((\pi x)_i - \pi \bar{x})^2 = \sum _{i=1}^{N} (x_i -  \bar{x})^2$$

Vemos entonces que

$$\mathbb{E}_{\pi}[((\pi x)_i - \pi \bar{x})((\pi x)_j - \pi \bar{x})] = \mathbb{E}_{\pi}[((\pi x)_i - \bar{x})((\pi x)_j - \bar{x})]$$

Sea $f: S^N \rightarrow \mathbb{R}$ definida por $f(\pi) = ((\pi x)_i - \bar{x})((\pi x)_j - \bar{x})$, luego tenemos

$$\mathbb{E}_{\pi}(f(\pi)) = \sum_{\pi_0 \in S^n} f(\pi_0) \mathbb{P}(\pi = \pi_0)$$

$$ = \sum_{\pi_0 \in S^N} f(\pi) \frac{1}{N!} = \frac{1}{N!} \sum_{\pi \in S^N} ((\pi x)_i - \bar{x})((\pi x)_j - \bar{x})$$

Como $\pi$ es una permutación, entonces $(\pi x)_i \neq (\pi x)_j$ para $i \neq j$ y además notamos que para $(x_i,x_j)$ fijos existen $(N-2)!$ permutaciones $\pi$ tales que $(x_i,x_j) = ((\pi x)_i, (\pi x)_j)$ para $i \neq j$, por lo tanto

$$\sum_{\pi \in S^N} ((\pi x)_i - \bar{x})((\pi x)_j - \bar{x}) = (N-2)!\sum_{i \neq j} (x_i - \bar{x})(x_j - \bar{x}) $$

Y entonces

$$\mathbb{E}_{\pi}[((\pi x)_i - \pi \bar{x})((\pi x)_j - \pi \bar{x})] =\frac{(N-2)!}{N!} \sum_{i \neq j} (x_i - \bar{x})(x_j - \bar{x})$$

$$ = \frac{1}{N(N-1)} [ (\sum_{k} x_k - \bar{x})^2 - \sum_{k} (x_k - \bar{x})^2]$$

$$ = \frac{-\sum_{k}(x_k - \bar{x})^2}{N(N-1)}$$ \newpage

Luego

$$\mathbb{E}_{\pi}(I) = \frac{N}{W} \frac{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} (\frac{-\sum_{k}(x_k - \bar{x})^2}{N(N-1)})}{\sum _{i=1}^{n} (x_i -  \bar{x})^2}$$ Y por lo tanto

$$\mathbb{E}_{\pi}(I) = - \frac{1}{N-1}$$

Concluimos que de aceptar la hipótesis nula, se espera que

$$\mathbb{E}(I) = - \frac{1}{N-1}$$

\newpage

## Anexo 6: Códigos para plots de modelos para generación de matriz de pesos estandarizada

Podemos hacer un plot de la estructura que posee el objeto "neighborhood list", para visualizar usamos funciones descritas en Analyzing US Census.

```{r, results='hide', warning=FALSE, message=FALSE}

# Filtramos para visualizar condado de Bronx
bronx <- nyc_data_prepped[str_detect(nyc_data_prepped$NAM, "Bronx"),]

# Guardamos su geometría (polygons)
bronx_geom <- st_geometry(bronx)

# Centroide de cada tramo censal y coordenadas
bronx_centroids = st_centroid(bronx_geom)
bronx_coordinates = st_coordinates(bronx_geom)

# Creamos neighbor list
bronx_nb_queen <- spdep::poly2nb(bronx)
bronx_nb_rook <- spdep::poly2nb(bronx, queen=FALSE)
```

**Bronx Queen Model**

```{r}
bronx_nb_queen
```

**Bronx Rook Model**

```{r}
bronx_nb_rook
```

\newpage

Para unir las dos imágenes en el mismo plot

```{r, results='hide', warning=FALSE, message=FALSE}
# save plot 
jpeg("image/bronx_neighborhood_rook_queen.jpg", width = 1000, height = 800)

# Preparamos plot de 2 imagenes
par(mfrow = c(1,2))

# Plot modelo Queen
plot(bronx_geom,
     main = "Queen",
     reset = FALSE,
     cex.main = 3)
plot(bronx_nb_queen, bronx_centroids,
     add = TRUE,
     col = 2,
     lwd = 1.5)

# Plot modelo Rook
plot(bronx_geom,
     main = "Rook",
     reset = FALSE,
     cex.main = 3)
plot(bronx_nb_rook, bronx_centroids,
     add = TRUE,
     col = 2,
     lwd = 1.5)

# close jpeg
dev.off()
```

```{r, echo = FALSE, fig.align = 'center', out.width="60%"}
knitr::include_graphics("image/bronx_neighborhood_rook_queen.jpg")
```

\newpage

## Anexo 7: Códigos para implementación de modelo GRF y visualización de R2 local

*Entrenamiento modelo GRF de SpatialML*

El código empleado para usar el paquete *SpatialML* en el entrenamiento del modelo es el siguiente

```{r, eval=FALSE, results='hide', message=FALSE}
# Paquetes
library(SpatialML)
library(sf)
library(tidyverse)

# Obtenemos centroides
nyc_data_prepped <- nyc_data_prepped %>% 
  mutate(lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),
         lat = map_dbl(geometry, ~st_centroid(.x)[[2]]))

# Creamos columna de coordenadas
coords <- nyc_data_prepped %>% 
  st_drop_geometry() %>% 
  select(lat,lon)

# Filtramos
nyc_grf_prepped <- nyc_data_prepped %>% 
  st_drop_geometry() %>%
  mutate(sqrt_med_value = sqrt(median_value)) %>% 
  select(!c(GEOID, NAM, residuals, lagged_residuals, lat, lon))

# Definimos formula para GRF
formula_grf <- "sqrt_med_value ~ median_rooms + median_income + 
pct_college + pct_foreign_born + pct_white + pct_black + 
pct_hispanic + pct_asian + median_age + percent_ooh  + 
pop_density"


# Optimizamos bandwitch
bwgrf <- grf.bw(formula = formula_grf,
                dataset = nyc_grf_prepped,
                kernel = "adaptive",
                coords = coords,
                bw.min = 98,
                bw.max = 98,
                step = 1,
                trees = 500,
                mtry = NULL,
                importance = "impurity",
                forests = FALSE,
                weighted = TRUE,
                verbose = TRUE)

# Entrena el modelo
grf_model <- grf(formula = formula_grf,
    dframe = nyc_grf_prepped,
    bw= 98,
    ntree = 500,
    mtry = 2,
    kernel = "adaptive",
    forests = TRUE,
    coords = coords)
```

\newpage

*Comparativa de R2 local entre modelos GWR y GRF*

Usamos ggplot para visualizar los 3 modelos sobre el mismo mapa y bajo la misma escala de colores

```{r, message=FALSE}
library(ggthemes)
# pivot then plot
(ggplot_comp_loc_r2 <- nyc_data_prepped_sp %>% 
  st_as_sf() %>% 
  pivot_longer(cols = c("spgwr", "gwmodel", "grf")) %>% 
  ggplot(aes(fill = value)) +
    geom_sf(color = NA) +
    scale_fill_fermenter(n.breaks = 8, palette = "RdBu", direction = 0) +
    ggthemes::theme_map(base_size = 8) +
    facet_wrap(~name) +
    labs(title = "Modelos GW: Comparativa R2 Local",
         fill = "Local R2") +
    theme(plot.title.position = 'plot',
          plot.title = element_text(hjust = 0.5,
                                    vjust = 3,
                                    size = 20),
          legend.position = c(0.20,-0.28),
          legend.key.height = unit(0.85, 'cm'),
          legend.key.width = unit(2, "cm"),
          legend.direction = "horizontal",
          legend.text = element_text(size=10),
          legend.title = element_text(size = 10),
          strip.text = element_text(size = 10)))

png('image/localR2GWM.png')
print(ggplot_comp_loc_r2)
dev.off()
```
